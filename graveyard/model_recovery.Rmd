---
title: "Model Recovery"
author: "Markus Steiner"
date: "22 6 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
library(tidyverse)
library(knitr)
library(yarrr)

```

## R Markdown

In the model recovery we simulated data of 60 participants x 3 environments for each of the four models (random model, natural mean model, reinforcement learning model, and sample extrapolation model). These data were then used to recover the parameters. Here is a table of the percent correct recoveries:

```{r echo = FALSE}
load("../data/Study1Data/useData/ParameterRecoveryOptim01.RData")
df_sim <- readRDS("../data/Study1Data/useData/ModelSimDat_All.rds")

subj_fits <- subj_fits %>%
  filter(model != "RLGoal")

dat <- dat %>%
  filter(model != "RLGoal")

####   Results for the other models

# Get the best model for each subject
model_best <- subj_fits %>%
  group_by(id) %>%
  summarise(
    N_models = sum(bic == min(bic)),       # Number of best fitting models (hopefully 1)
    model_best = model[bic == min(bic)][1],
    model_best_bic = bic[bic == min(bic)][1],
    model_best_Imp = pars_Imp_mle[bic == min(bic)][1],
    model_best_Choice = pars_Choice_mle[bic == min(bic)][1]
  )


# Combine actual participant conditions with best model

model_best <- dat %>%
  left_join(model_best)   # Add modelling results

datn <- model_best %>%
  group_by(id) %>%
  summarise(
    pars_Choice = pars_Choice[1],
    pars_Imp = pars_Imp[1],
    model_best_Imp = model_best_Imp[1],
    model_best_Choice = model_best_Choice[1],
    model = model[1],
    model_best = model_best[1]
  )

model_best$corr_class <- as.numeric(model_best$model == model_best$model_best)

model_best <- model_best[order(model_best$id),]
datn <- datn[order(datn$id),]
subj_fits <- subj_fits[order(subj_fits$id),]


kable(round(prop.table(table(model_best$model, model_best$model_best)/250, 1), 2))



```


The first three models are well recovered, but the Sample Extrapolation model (SampEx) is only in 58% of the cases correctly recovered, and otherwise is classified best by the Random model. Here I try to explore a bit the reasons for why this might be the case before I start creating new models. The following two figures show the histograms of the choice (i.e. $\phi$) and impression (i.e. $N$, the number of trials sampled) parameters of the SampEx model when it was correctly recovered, and when it was incorrectly recovered as Random.

```{r echo = FALSE}
check_ids <- unique(model_best$id[model_best$model == "SampEx_Int_Goal" & model_best$model_best == "Random"])
check_ids2 <- unique(model_best$id[model_best$model == "SampEx_Int_Goal" & model_best$model_best == "SampEx_Int_Goal"])

par(mfrow = c(2, 1))
hist(subj_fits$pars_Choice_mle[subj_fits$id %in% check_ids & subj_fits$model == "SampEx_Int_Goal"], xlim = c(0, 3), main = "Recovered Choice Pars Wrongly Recovery", xlab = "Choice Parameter Phi", breaks = 20,
     ylim = c(0, 20), col = "black")
hist(subj_fits$pars_Choice_mle[subj_fits$id %in% check_ids2 & subj_fits$model == "SampEx_Int_Goal"], xlim = c(0, 3), main = "Recovered Choice Pars Correctly Recovery", xlab = "Choice Parameter Phi", breaks = 20,
     ylim = c(0, 20), col = "black")
```

So for the choice parameter $\phi$ the ones with relatively low parameter values were recovered wrongly. Let's check how the recovered parameters relate to the true ones.

```{r}
par(mfrow = c(1, 1))
plot(datn$pars_Choice[datn$id %in% check_ids],
     subj_fits$pars_Choice_mle[subj_fits$id %in% check_ids & subj_fits$model == "SampEx_Int_Goal"],
     main = "True and Recovered Model Choice Parameters - Wrongly Recovered",
     xlab = "True Phi-Parameter Value",
     ylab = "Recovered Phi-Parameter Value",
     pch = 16,
     ylim = c(0, 2.5),
     xlim = c(0, 2.5))
abline(0, 1, lty = 2)

```

So for the wrongly recovered models the recovered parameter values don't seem to be much related with the true parameter values with which the data was created. Let's check the same relationship for the correctly recovered models.

```{r}
par(mfrow = c(1, 1))
plot(datn$pars_Choice[datn$id %in% check_ids2],
     subj_fits$pars_Choice_mle[subj_fits$id %in% check_ids2 & subj_fits$model == "SampEx_Int_Goal"],
     main = "True Recovered Model Choice Parameters - Correctly Recovered",
     xlab = "True Phi-Parameter Value",
     ylab = "Recovered Phi-Parameter Value",
     pch = 16,
     ylim = c(0, 2.5),
     xlim = c(0, 2.5))
abline(0, 1, lty = 2)


```

Ok here we have a stronger relationship. Maybe it would be a good idea to check an $\epsilon$-greedy choice rule that we don't multiply numbers that are close to 0 with each other. So just for comparison, let's also look at the plot for the $\phi$ parameter of the reinforcement learning model:


```{r}

check_ids3 <- unique(model_best$id[model_best$model == "RL" & model_best$model_best == "RL"])

par(mfrow = c(1, 1))
plot(datn$pars_Choice[datn$id %in% check_ids3 & datn$model == "RL"],
     datn$model_best_Choice[datn$id %in% check_ids3 & datn$model == "RL"],
     main = "True and Recovered Model Choice Parameters - RL Model",
     xlab = "True Phi-Parameter Value",
     ylab = "Recovered Phi-Parameter Value",
     pch = 16,
     ylim = c(0, 8),
     xlim = c(0, 8))
abline(0, 1, lty = 2)

```

Here parameter values seem to be recovered relatively well. There exists an obvious difference in the choice parameter values of the correctly and incorrectly recovered data generated by the Sample Extrapolation model. So let's look at the impression parameter, that is, how many samples are drawn from the memory to form a judgement.

First let's check the histograms of the impression parameters of the wrongly recovered models and the correctly recovered models.


```{r echo = FALSE}
par(mfrow = c(2, 1))
hist(subj_fits$pars_Imp_mle[subj_fits$id %in% check_ids & subj_fits$model == "SampEx_Int_Goal"], xlim = c(0, 15), main = "Recovered Imp Pars Wrongly Recovery", xlab = "Impression Parameter N", breaks = 15,
     ylim = c(0, 20), col = "black")
hist(subj_fits$pars_Imp_mle[subj_fits$id %in% check_ids2 & subj_fits$model == "SampEx_Int_Goal"], xlim = c(0, 15), main = "Recovered Imp Pars Correctly Recovery", xlab = "Impression Parameter N", breaks = 15,
     ylim = c(0, 20), col = "black")
```

The impression parameter distribution of the wrongly recovered models is, like the one of the choice parameters, shifted to the left. So to correctly recover the models we seem to need at least an N of 3 or 4 (which is larger than the empirical mean $N$ in our experiment data), and a sufficiently large choice sensitivity. Let's now check how well the parameters were recovered, separated for whether the model was wrongly or correctly recovered.

```{r}
par(mfrow = c(1, 1))
plot(datn$pars_Imp[datn$id %in% check_ids & datn$model == "SampEx_Int_Goal"],
     subj_fits$pars_Imp_mle[subj_fits$id %in% check_ids & subj_fits$model == "SampEx_Int_Goal"],
     main = "True Recovered Model Imp Parameters - Wrongly Recovered",
     xlab = "True N-Parameter Value",
     ylab = "Recovered N-Parameter Value",
     pch = 16,
     ylim = c(0, 15),
     xlim = c(0, 15))
abline(0, 1, lty = 2)

```

So in the wrongly recovered models, the impression parameter $N$ is often underestimated and there doesn't seem to be much of a relationship.

```{r}
par(mfrow = c(1, 1))
plot(datn$pars_Imp[datn$id %in% check_ids2 & datn$model == "SampEx_Int_Goal"],
     subj_fits$pars_Imp_mle[subj_fits$id %in% check_ids2 & subj_fits$model == "SampEx_Int_Goal"],
     main = "True Recovered Model Imp Parameters - Correctly Recovered",
     xlab = "True N-Parameter Value",
     ylab = "Recovered N-Parameter Value",
     pch = 16,
     ylim = c(0, 15),
     xlim = c(0, 15))
abline(0, 1, lty = 2)

```

Here it looks much more reasonable. But it seems as if the SampEx model, at least in the current form, needs a relatively high impression parameter, to be correctly recovered. As a comparison, let's look at the impression parameters from the reinforcement learning model.



```{r}

par(mfrow = c(1, 1))
plot(datn$pars_Imp[datn$id %in% check_ids3 & datn$model == "RL"],
     subj_fits$pars_Imp_mle[subj_fits$id %in% check_ids3 & subj_fits$model == "RL"],
     main = "True and Recovered Model Imp Parameters - RL Model",
     xlab = "True Alpha-Parameter Value",
     ylab = "Recovered Alpha-Parameter Value",
     pch = 16,
     ylim = c(0, 1),
     xlim = c(0, 1))
abline(0, 1, lty = 2)

```


So this is how it is supposed to look. So the SampEx model definitely needs to be revised.

Now lets check the correlation of the parameters for the wrongly recovered model.

```{r}
par(mfrow = c(1, 1))
plot(subj_fits$pars_Choice_mle[subj_fits$id %in% check_ids & subj_fits$model == "SampEx_Int_Goal"],
     subj_fits$pars_Imp_mle[subj_fits$id %in% check_ids & subj_fits$model == "SampEx_Int_Goal"],
     main = "SampEx Parameter Correlations - Wrongly Recovered",
     xlab = "Recovered Phi Parameter Value",
     ylab = "Recovered N-Parameter Value",
     pch = 16,
     ylim = c(0, 15),
     xlim = c(0, 1.5))
abline(0, 1, lty = 2)
text(1.3, 14.5, paste("cor =", round(cor(subj_fits$pars_Choice_mle[subj_fits$id %in% check_ids & subj_fits$model == "SampEx_Int_Goal"], subj_fits$pars_Imp_mle[subj_fits$id %in% check_ids & subj_fits$model == "SampEx_Int_Goal"]), 3)))

```


Ok so parameters in the wrongly recovered model are not correlated. Let's see how this looks for the correctly recovered model.



```{r}
par(mfrow = c(1, 1))
plot(subj_fits$pars_Choice_mle[subj_fits$id %in% check_ids2 & subj_fits$model == "SampEx_Int_Goal"],
     subj_fits$pars_Imp_mle[subj_fits$id %in% check_ids2 & subj_fits$model == "SampEx_Int_Goal"],
     main = "SampEx Parameter Correlations - Correctly Recovered",
     xlab = "Recovered Phi Parameter Value",
     ylab = "Recovered N-Parameter Value",
     pch = 16,
     ylim = c(0, 15),
     xlim = c(0, 3))
abline(0, 1, lty = 2)
text(2.5, 14.5, paste("cor =", round(cor(subj_fits$pars_Choice_mle[subj_fits$id %in% check_ids2 & subj_fits$model == "SampEx_Int_Goal"], subj_fits$pars_Imp_mle[subj_fits$id %in% check_ids2 & subj_fits$model == "SampEx_Int_Goal"]), 3)))
```


Here the correlation is a bit higher but still not a huge correlation.